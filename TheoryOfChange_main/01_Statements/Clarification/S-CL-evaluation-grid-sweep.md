---
id: stmt.cl-evaluation-grid-sweep
type: CL
title: Evaluation grids & multi-seed sweeps anchor performance comparisons
dependencies: ["[[01_Statements/Clarification/S-CL-compute-fairness-contract]]"]
parents: []
successors: []
concepts: ["[[02_Concepts/C-benchmarks-audit]]"]
symbols_used: []
sources:
  - path: TheoryOfChange/00_Meta/AI_RecursiveChats_slim/AI_17_Spiral9.md:304-331
flags: []
tags: [evaluation, verification, "type/CL"]
---
# Evaluation grids & multi-seed sweeps anchor performance comparisons

Every comparative claim must define the evaluation grid (problem sizes/seeds/maze layouts/bandit params) ahead of time, preregister the seeds, and run multi-seed sweeps with distribution CSVs and CI plots. Without this infrastructure, CO's reported advantage could be attributed to lucky seeds or cherry-picked configs. This is the workflow implied in AI_17's H1-H3 agent plan.




































































<!-- BEGIN:AUTOGEN:REFERENCED_BY -->
## Referenced By
- [[01_Statements/Clarification/S-CL-co-lateg-plan]]
<!-- END:AUTOGEN:REFERENCED_BY -->







































































































































<!-- BEGIN:AUTOGEN:RELATIONSHIPS -->
## Relationships

- Concepts: [[02_Concepts/C-benchmarks-audit]]
- Dependencies: [[01_Statements/Clarification/S-CL-compute-fairness-contract]]
<!-- END:AUTOGEN:RELATIONSHIPS -->



































































